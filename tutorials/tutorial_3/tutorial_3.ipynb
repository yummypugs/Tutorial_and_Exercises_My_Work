{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Tutorial 3: Loss and Optimization'\n",
        "author: Jiawei Li\n",
        "editor:\n",
        "  render-on-save: true\n",
        "execute:\n",
        "  warning: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Housing Prices\n",
        "\n",
        "For this tutorial, let us use [Ames Housing dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) on Kaggle to explore the housing prices. One simplest way to model housing prices is a \"multiple\" measure between between housing area and price:\n",
        "\n",
        "$$\n",
        "\\text{price} = \\beta \\times \\text{area}\n",
        "$$\n",
        "\n",
        "First, we read the data and plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "housing = pd.read_csv(\"train.csv\")[[\"GrLivArea\", \"SalePrice\"]]\n",
        "\n",
        "alt.Chart(housing).mark_circle().encode(\n",
        "    x = \"GrLivArea\",\n",
        "    y = \"SalePrice\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss\n",
        "\n",
        "We use $y$ to denote the actual price, $x$ to denote the area, $\\beta$ is the parameter (or the multiple in this case), $\\epsilon$ is the irreducible error that collects all the unmodeled parts of our data. We use superscript $(i)$ to denote the index of the data. For example, the first row of data is:\n",
        "\n",
        "$$\n",
        "y^{(0)} = \\beta x^{(0)} + \\epsilon^{(0)}\n",
        "$$\n",
        "\n",
        "And $\\hat{y}$ is the predicted price and $\\hat{\\beta}$ is the predicted parameter.\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(0)} = \\hat{\\beta} x^{(0)}\n",
        "$$\n",
        "\n",
        "It can be easily vectorized. $y$ is the column of housing price,$\\begin{bmatrix}\n",
        "   y^{(0)} \\\\\n",
        "   y^{(1)} \\\\\n",
        "   \\vdots   \\\\\n",
        "   y^{(n)} \\\\\n",
        "\\end{bmatrix}$. $x$ is the column of housing area, $\\begin{bmatrix}\n",
        "   x^{(0)} \\\\\n",
        "   x^{(1)} \\\\\n",
        "   \\vdots   \\\\\n",
        "   x^{(n)} \\\\\n",
        "\\end{bmatrix}$. We have:\n",
        "\n",
        "$$\n",
        "y = \\beta x + \\epsilon\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\hat{\\beta} x\n",
        "$$\n",
        "\n",
        "Our goal is to find $\\beta^*$ that minimizes the mean of the squared errors.\n",
        "\n",
        "$$\n",
        "\\beta^*:=\\underset{\\hat{\\beta}}{\\operatorname{argmin}}\\ J(\\hat{\\beta})\n",
        "$$\n",
        "\n",
        "The mean of the squared errors is used as a loss or cost function:\n",
        "\n",
        "$$\n",
        "J(\\hat{\\beta})\n",
        "= \\text{MSE}\n",
        "= \\frac{1}{n} (y - \\hat{y})^\\top (y - \\hat{y})\n",
        "= \\frac{1}{n} \\sum_{i=1}^n\\left[y^{(i)}-\\hat{y}^{(i)}\\right]^2\n",
        "= \\frac{1}{n} \\sum_{i=1}^n\\left[y^{(i)}-\\hat{\\beta} x^{(i)}\\right]^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_mse(beta):\n",
        "    \"\"\"\n",
        "    Calculate the mean of the squared errors of the housing area and price given beta.\n",
        "    \"\"\"\n",
        "    squared_error = (housing[\"SalePrice\"] - housing[\"GrLivArea\"] * beta) ** 2\n",
        "    mean_squared_error = squared_error.mean()\n",
        "    return mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "calculate_mse(80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot loss function in respect to the parameter $\\beta$, the output is known as the loss landscape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_mse(low=0, high=500):\n",
        "    \"\"\"\n",
        "    Plot the mean of the squared errors for a range of betas.\n",
        "    \"\"\"\n",
        "    betas = np.arange(low, high, 1)\n",
        "    losses = np.vectorize(calculate_mse)(betas)\n",
        "    source = pd.DataFrame({\"beta\": betas, \"loss\": losses})\n",
        "    chart = alt.Chart(source).mark_line().encode(\n",
        "        x = \"beta\",\n",
        "        y = \"loss\"\n",
        "    )\n",
        "    return chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_mse(low=0, high=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Derivative\n",
        "\n",
        "We can rewrite the loss function $J(\\hat{\\beta})$ in a more friendly format:\n",
        "\n",
        "$$\n",
        "J(\\hat{\\beta}) = \\frac{1}{2} \\sum_{i=1}^n\\left[y^{(i)}-\\hat{\\beta} x^{(i)}\\right]^2\n",
        "$$\n",
        "\n",
        "The first derivative of $J(\\hat{\\beta})$ becomes:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\hat{\\beta}} J(\\hat{\\beta}) = -\\sum_{i=1}^n\\left[y^{(i)}-\\hat{\\beta} x^{(i)}\\right]x^{(i)}\n",
        "$$\n",
        "\n",
        "We can use gradient descent to reach the minimum of loss function by taking the steps towards to direction indicated by the first derivative. For linear regression, a solution can be directly calculated because the loss function is convex, which can be proved by the second derivative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_mse_prime(beta):\n",
        "    \"\"\"\n",
        "    Calculate the first derivative of mean squread error given beta.\n",
        "    \"\"\"\n",
        "    mse_prime = -((housing[\"SalePrice\"] - housing[\"GrLivArea\"] * beta) * housing[\"GrLivArea\"]).sum()\n",
        "    return mse_prime\n",
        "\n",
        "def plot_mse_prime(low=0, high=500):\n",
        "    \"\"\"\n",
        "    Plot the first derivative of mean squread error given beta.\n",
        "    \"\"\"\n",
        "    betas = np.arange(low, high, 1)\n",
        "    prime = np.vectorize(calculate_mse_prime)(betas)\n",
        "    source = pd.DataFrame({\"beta\": betas, \"loss\": prime})\n",
        "    chart = alt.Chart(source).mark_line().encode(\n",
        "        x = \"beta\",\n",
        "        y = \"loss\"\n",
        "    )\n",
        "    return chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_mse_prime(low=0, high=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second Derivative\n",
        "\n",
        "From the plot, we can already tell the first derivative is a straight line. This can be expressed as the second derivative of the loss function:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2}{\\partial^2 \\hat{\\beta}} J(\\hat{\\beta}) = \\sum_{i=1}^n \\left[x^{(i)}\\right]^2\n",
        "$$\n",
        "\n",
        "Which is a positive constant number.\n",
        "\n",
        "## Optimization\n",
        "\n",
        "The second derivative gives us indication that the loss function is convex. So we can set first derivative to zero to get the global minimum.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "   \\frac{\\partial}{\\partial \\beta^*} J(\\beta^*) &= 0 \\\\\n",
        "   \\sum_{i=1}^n\\left[y^{(i)}-\\beta^* x^{(i)}\\right] x^{(i)} &= 0 \\\\\n",
        "   \\sum_{i=1}^n x^{(i)}y^{(i)} -\\beta^* [x^{(i)}]^2 &= 0 \\\\\n",
        "   \\sum_{i=1}^n x^{(i)}y^{(i)} &= \\sum_{i=1}^n \\beta^* [x^{(i)}]^2 \\\\\n",
        "   \\beta^* &= \\frac{\\sum_{i=1}^n x^{(i)}y^{(i)}}{\\sum_{i=1}^n [x^{(i)}]^2}  \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "On your exercise, the matrix form is used to accommodate intercept term and more features. For example, if we add an intercept to our equation:\n",
        "\n",
        "$$\n",
        "\\text{price} = \\beta_0 + \\beta_1 \\times \\text{area}\n",
        "$$\n",
        "\n",
        "We can rewrite $\\beta$ as $\\begin{bmatrix}\n",
        "   \\beta_0 \\\\\n",
        "   \\beta_1\n",
        "\\end{bmatrix}$, the feature $X$ as $\\begin{bmatrix}\n",
        "   1 & x^{(0)} \\\\\n",
        "   1 & x^{(1)} \\\\\n",
        "   \\vdots & \\vdots   \\\\\n",
        "   1 & x^{(n)} \\\\\n",
        "\\end{bmatrix}$, and the equation becomes:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X \\hat{\\beta}\n",
        "$$\n",
        "\n",
        "From [properties of matrix multiplication](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:properties-of-matrix-multiplication/a/properties-of-matrix-multiplication), we know that $(AB)^\\top=B^\\top A^\\top$ and $A(B+C) = AB + AC$. The loss function becomes:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "J(\\hat{\\beta}) &= \\frac{1}{2} \\left[ (\\hat{y} - X \\hat{\\beta})^\\top (\\hat{y} - X \\hat{\\beta}) \\right] \\\\\n",
        "&= \\frac{1}{2} \\left[(y - X\\hat{\\beta})^\\top y - (y - X\\hat{\\beta})^\\top X\\hat{\\beta} \\right]\\\\\n",
        "&= \\frac{1}{2} \\left[y^\\top y - (X\\hat{\\beta})^\\top y - y^\\top X\\hat{\\beta} + (X\\hat{\\beta})^\\top X\\hat{\\beta} \\right]\\\\\n",
        "&= \\frac{1}{2} \\left[y^\\top y - \\hat{\\beta}^\\top X^\\top y - y^\\top X\\hat{\\beta} + \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The first derivative to the loss function is the Jacobian matrix:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\hat{\\beta}} J(\\hat{\\beta}) &=\\nabla_{\\hat{\\beta}} \\frac{1}{2} \\left[y^\\top y - \\hat{\\beta}^\\top X^\\top y - y^\\top X\\hat{\\beta} + \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right] \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The first item $y^\\top y$ can be ignored since it has nothing to do with $\\beta$. The second and third term are both scaler and are transpose of each other. Therefore these two terms can be combined:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\hat{\\beta}} J(\\hat{\\beta}) &= \\nabla_{\\hat{\\beta}} \\frac{1}{2} \\left[y^\\top y - \\hat{\\beta}^\\top X^\\top y - y^\\top X\\hat{\\beta} + \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right] \\\\\n",
        "\\nabla_{\\hat{\\beta}} J(\\hat{\\beta}) &= \\nabla_{\\hat{\\beta}} \\frac{1}{2} \\left[- 2 (X^\\top y)^\\top \\hat{\\beta} + \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right] \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "It is trivial to see that $\\nabla_{\\hat{\\beta}} \\left[ 2 (X^\\top y)^\\top \\hat{\\beta} \\right] = 2 X^\\top y$.\n",
        "\n",
        "For $\\nabla_{\\hat{\\beta}} \\left[ \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right]$, suppose we have a symetric matrix $A=\\begin{bmatrix}\n",
        "a_{11} & a_{12} \\\\\n",
        "a_{12} & a_{22}\n",
        "\\end{bmatrix}$ in place of $X^\\top X$. The original equation becomes $\\hat{\\beta}^\\top A \\hat{\\beta}$, which can be decomposed into:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\beta}^\\top A \\hat{\\beta}\n",
        "=&\n",
        "\\begin{bmatrix}\n",
        "\\hat{\\beta}_0 & \\hat{\\beta}_1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} \\\\\n",
        "a_{12} & a_{22}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\hat{\\beta}_0 \\\\\n",
        "\\hat{\\beta}_1\n",
        "\\end{bmatrix}\\\\\n",
        "=&\n",
        "\\begin{bmatrix}\n",
        "\\hat{\\beta}_0 & \\hat{\\beta}_1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_{11} \\hat{\\beta}_0 + a_{12} \\hat{\\beta}_1 \\\\\n",
        "a_{12} \\hat{\\beta}_0 + a_{22} \\hat{\\beta}_1\n",
        "\\end{bmatrix}\\\\\n",
        "=& \\hat{\\beta}_0 (a_{11} \\hat{\\beta}_0+a_{12} \\hat{\\beta}_1) +\\hat{\\beta}_1 (a_{12} \\hat{\\beta}_0+a_{22} \\hat{\\beta}_1) \\\\\n",
        "=& a_{11} \\hat{\\beta}_0^2 + a_{22} \\hat{\\beta}_1^2+ 2 a_{12} \\hat{\\beta}_0 \\hat{\\beta}_1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The Jacobian matrix of $\\hat{\\beta}^\\top A \\hat{\\beta}$ is therefore:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\hat{\\beta}}\\hat{\\beta}^TA\\hat{\\beta} = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial \\hat{\\beta}^TA\\hat{\\beta}}{\\partial \\hat{\\beta}_0}\\\\ \n",
        "\\frac{\\partial \\hat{\\beta}^TA\\hat{\\beta}}{\\partial \\hat{\\beta}_1}\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "2a_{11}\\hat{\\beta}_0 + 2a_{12}\\hat{\\beta}_1\\\\ \n",
        "2a_{12}\\hat{\\beta}_0 + 2a_{22}\\hat{\\beta}_1\n",
        "\\end{bmatrix} = 2A\\hat{\\beta}\n",
        "$$\n",
        "\n",
        "And the Jacobian matrix of $\\hat{\\beta}^\\top X^\\top X \\hat{\\beta}$ is $2X^\\top X\\hat{\\beta}$. we have\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\hat{\\beta}} J(\\hat{\\beta}) &= \\nabla_{\\hat{\\beta}} \\frac{1}{2} \\left[- 2 (X^\\top y)^\\top \\hat{\\beta} + \\hat{\\beta}^\\top X^\\top X\\hat{\\beta} \\right] \\\\\n",
        "&= - X^\\top y + X^\\top X\\hat{\\beta}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To show that the loss function is convex you need to show that the second derivative of the function, known as the Hessian matrix, is positive semi-definite. This step can be skipped. Set the Jacobian matrix to 0, you will get the normal equation.\n",
        "\n",
        "# Further Reading\n",
        "\n",
        "[Linear Regression - MLU](https://mlu-explain.github.io/linear-regression/)  \n",
        "[Linear Algebra Review and Reference](https://cs229.stanford.edu/summer2020/cs229-linalg.pdf)  \n",
        "[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)  \n",
        "[Lecture Notes 1 - CS229](http://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf)  \n",
        "[最小二乘法线性回归：矩阵视角 (Chinese)](https://iewaij.github.io/introDataScience/OLS.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}