{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Solution 2: Dimensionality Reduction'\n",
        "author: Jiawei Li\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensionality Reduction\n",
        "(a) What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "One motivation is to reduce redundant data. Sometimes, a few columns might have a perfect linear relationship, resulting in collinearity. For example, for a car speed dataset, one column is kilometre/hour and another column is mile/hour. These two columns has a perfect linear relationship that 1 kilometre/hour is roughly 0.62 mile/hour. PCA can eliminate such redundancy.\n",
        "\n",
        "Another motivation is to reduce computational and storage cost. Reduced dimensionality means reduced workload and size for computing and storing.\n",
        "\n",
        "The last motivation is related to the curse of dimensionality.\n",
        "\n",
        "The main drawback is that we may lose important information of the data and thus lose predictive performance if we use such data to train models.\n",
        "\n",
        "(b) Describe the Johnson-Lindenstrauss lemma.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The Johnson-Lindenstrauss lemma claims that, if you have $N$ points with $k$ dimensions, you can always map them into $d$ dimensions such that pairwise distances are preserved up to a small constant factor.\n",
        "\n",
        "(c) What is the curse of dimensionality?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The curse of dimensionality refers to various phenomena that one’s intuitions about how data structures, similarity measures, and algorithms behave in low dimensions do not generalize to higher dimensions. One example is nearest neighbours. In high dimensional space, neighbouring and non-neighbouring points appear to be equally far away, thus making nearest neighbours meaningless. It also occurs in many other geometric problems, such as [Stein's paradox](https://joe-antognini.github.io/machine-learning/steins-paradox) and [kissing numbers](http://www.ams.org/notices/200408/fea-pfender.pdf).\n",
        "\n",
        "# Principal Component Analysis\n",
        "Suppose we have a dataset with 4 points:\n",
        "\n",
        "$$\n",
        "\\mathcal{D}=\\{(1,5),(0,6),(-7,0),(-6,-1)\\}\n",
        "$$\n",
        "\n",
        "(a) Plot the dataset and try to guess two principal components $(k=2)$.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize the data frame\n",
        "data = pd.DataFrame([(1, 5), (0, 6), (-7, 0), (-6, -1)], columns=[\"x\", \"y\"])\n",
        "# Center the data so each feature has zero mean\n",
        "data = data - data.mean()\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can guess that the projected line (1st component) should be in the direction of $y = x$ to minimize the distance between the points and the projection. The 2nd component should be perpendicular to the 1st component, thus should be in the direction $y = -x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base = (\n",
        "    alt.Chart(data)\n",
        "    .mark_circle()\n",
        "    .encode(\n",
        "        x=alt.X(\"x\", scale=alt.Scale(domain=(-5, 5))),\n",
        "        y=alt.Y(\"y\", scale=alt.Scale(domain=(-5, 5))),\n",
        "    )\n",
        "    .properties(width=300, height=300)\n",
        ")\n",
        "base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_1 = pd.DataFrame([(5, 5), (-5, -5)], columns=[\"x\", \"y\"])\n",
        "data_2 = pd.DataFrame([(3, -3), (-3, 3)], columns=[\"x\", \"y\"])\n",
        "component_1 = alt.Chart(data_1).mark_line(color=\"grey\", size=1).encode(x=\"x\", y=\"y\")\n",
        "component_2 = alt.Chart(data_2).mark_line(color=\"lightgrey\", size=1).encode(x=\"x\", y=\"y\")\n",
        "chart = base + component_1 + component_2\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(b) Compute the empirical covariance matrix, its eigenvalues and eigenvectors. Do the eigenvectors correspond to your guess of principal components? Please do not forget the assumptions of PCA. (The dataset should be centred, and we want unit eigenvectors.)\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Consturct data in row format\n",
        "D = data.to_numpy().transpose()\n",
        "# Calculate the covariance matrix\n",
        "S = D @ D.T\n",
        "# Calculate the eigenvalues and eigenvectors\n",
        "l, v = np.linalg.eigh(S)\n",
        "# Sort eigenvectors according to eigenvalues\n",
        "idx = np.argsort(l)[::-1]\n",
        "l = l[idx]\n",
        "v = v[:,idx]\n",
        "# Project\n",
        "data_eigenvector_1 = pd.DataFrame(np.vstack([v[:, 0]*6, v[:, 0]*-6]), columns=[\"x\", \"y\"])\n",
        "data_eigenvector_2 = pd.DataFrame(np.vstack([v[:, 1]*3, v[:, 1]*-3]), columns=[\"x\", \"y\"])\n",
        "component_eigenvector_1 = alt.Chart(data_eigenvector_1).mark_line(color=\"grey\", size=1).encode(x=\"x\", y=\"y\")\n",
        "component_eigenvector_2 = alt.Chart(data_eigenvector_2).mark_line(color=\"lightgrey\", size=1).encode(x=\"x\", y=\"y\")\n",
        "chart = base + component_eigenvector_1 + component_eigenvector_2\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the projection from eigenvectors is close to our guess. You can also use the SVD decomposition method, as we discussed in tutorial 2. And again, using the row format for `D` is an arbitrary choice, feel free to do your own version of PCA using the column format.\n",
        "\n",
        "# Random Projection\n",
        "\n",
        "(a) Implement a random-projection method in python. The random projection $X^{\\mathrm{RP}} \\in \\mathbb{R}^{k \\times N}$ of $N$ samples $X \\in \\mathbb{R}^{d \\times N}$ is given by\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "$$\n",
        "X^{\\mathrm{RP}}=R X .\n",
        "$$\n",
        "\n",
        "A computationally efficient way to generate $R \\in \\mathbb{R}^{k \\times d}$ has been proposed by Achlioptas (2001). Generate the components of $R$ according to\n",
        "\n",
        "$$\n",
        "R_{i j}=\\sqrt{3} \\times \\begin{cases}+1 & \\text { with probability } \\frac{1}{6} \\\\ 0 & \\text { with probability } \\frac{2}{3} \\\\ -1 & \\text { with probability } \\frac{1}{6}\\end{cases}\n",
        "$$\n",
        "\n",
        "Note that $X \\in \\mathbb{R}^{k \\times N}$ means that $X$ is in row format. To use $X$ in column format, let us rewrite the transformation. Given $X \\in \\mathbb{R}^{N \\times k}$, the random projection $X^{\\mathrm{RP}} \\in \\mathbb{R}^{N \\times d}$ is\n",
        "\n",
        "$$\n",
        "X^{\\mathrm{RP}}= X R .\n",
        "$$\n",
        "\n",
        "$R_{ij}$ specified in the exercise is a probability density function. To preserve distances, we need further scale it by $\\sqrt{d}$.\n",
        "\n",
        "$$\n",
        "R_{ij}=\\frac{\\sqrt{3}}{\\sqrt{d}} \\times \\begin{cases}+1 & \\text { with probability } \\frac{1}{6} \\\\ 0 & \\text { with probability } \\frac{2}{3} \\\\ -1 & \\text { with probability } \\frac{1}{6}\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from itertools import product\n",
        "\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_rcv1\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_random_projection(k, d):\n",
        "    \"\"\"\n",
        "    Generate an efficient random matrix with the shape of k × d according to Achlioptas (2001).\n",
        "\n",
        "    References:\n",
        "        Achlioptas, Dimitris. 2003. Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins. Journal of Computer and System Sciences 66 (4): 671-87. https://doi.org/10.1016/S0022-0000(03)00025-4.\n",
        "    \"\"\"\n",
        "    return np.random.choice(\n",
        "        [-1, 0, 1],\n",
        "        size=(k, d),\n",
        "        p=[1 / 6, 2 / 3, 1 / 6]\n",
        "        ) * np.sqrt(3) / np.sqrt(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "generate_random_projection(5, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(b) Estimate the reduced dimension $k$ according to Theorem 2 in Achlioptas (2001). Try out different values of $\\beta$. Discuss and visualize the connection of the estimation of $k$ to the Johnson-Lindenstrauss lemma.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Given an arbitrary set of $n$ points in $\\mathbb{R}^k$, represented as an $n \\times k$ matrix, and $\\epsilon, \\beta > 0$, the reduced dimension according to Achlioptas (2001) is\n",
        "\n",
        "$$\n",
        "d=\\frac{4+2 \\beta}{\\epsilon^2 / 2-\\epsilon^3 / 3} \\log n\n",
        "$$\n",
        "\n",
        "The parameter $\\epsilon$ controls the desired accuracy in distance preservation, while $\\beta$ controls the projection’s probability of success. If we set $\\beta=0$, it is exactly the same as the Johnson-Lindenstrauss lemma, which takes the following form:\n",
        "\n",
        "$$\n",
        "d=\\frac{4}{\\epsilon^2 / 2-\\epsilon^3 / 3} \\log n\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def achlioptas_min_dim(eps, beta, n):\n",
        "    \"\"\"\n",
        "    Return the reduced dimension d according to Achlioptas (2001).\n",
        "\n",
        "    References:\n",
        "        Achlioptas, Dimitris. 2003. Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins. Journal of Computer and System Sciences 66 (4): 671-87. https://doi.org/10.1016/S0022-0000(03)00025-4.\n",
        "    \"\"\"\n",
        "    d = (4 + 2 * beta) / (eps**2 / 2 - eps**3 / 3) * np.log(n)\n",
        "    return d\n",
        "\n",
        "\n",
        "def johnson_lindenstrauss_min_dim(eps, n):\n",
        "    \"\"\"\n",
        "    Return the reduced dimension k according to Johnson-Lindenstrauss lemma, which is a special case of `achlioptas_min_dim()`.\n",
        "\n",
        "    References:\n",
        "        Dasgupta, Sanjoy, and Anupam Gupta. An Elementary Proof of the Johnson-Lindenstrauss Lemma.\n",
        "    \"\"\"\n",
        "    beta = 0\n",
        "    return achlioptas_min_dim(eps, beta, n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "achlioptas_min_dim(eps=0.1, beta=0.2, n=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "johnson_lindenstrauss_min_dim(eps=0.1, n=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# range of admissible distortions\n",
        "eps_range = np.linspace(0.1, 0.99, 5)\n",
        "\n",
        "# range of number of samples to embed\n",
        "n_range = np.logspace(1, 9, 9)\n",
        "\n",
        "# range of betas for projection control\n",
        "beta_range = np.array([0, 0.5])\n",
        "\n",
        "# Create a grid using product function\n",
        "data = pd.DataFrame(product(eps_range, n_range, beta_range), columns=[\"eps\", \"n\", \"beta\"])\n",
        "data[\"min_dim\"] = data.apply(lambda x: achlioptas_min_dim(eps=x[\"eps\"], beta=x[\"beta\"], n=x[\"n\"]), axis=1)\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that increasing $ε$ leads to decreasing $d$ when $β$ is set at 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "alt.Chart(data[data.beta == 0]).mark_line().encode(\n",
        "    x=alt.X(\"n:Q\", axis=alt.Axis(tickCount=10, format=\"s\"), scale=alt.Scale(type=\"log\")),\n",
        "    y=alt.Y(\"min_dim:Q\", axis=alt.Axis(tickCount=10, format=\"s\"), scale=alt.Scale(type=\"log\")),\n",
        "    color=\"eps\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also clear that increasing $β$ leads to increasing $d$ to preserve $ε$ at 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "alt.Chart(data[data.eps == 0.1]).mark_line().encode(\n",
        "    x=alt.X(\"n:Q\", axis=alt.Axis(tickCount=10, format=\"s\"), scale=alt.Scale(type=\"log\")),\n",
        "    y=alt.Y(\"min_dim:Q\", axis=alt.Axis(tickCount=10, format=\"s\"), scale=alt.Scale(type=\"log\")),\n",
        "    color=\"beta\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(c) Download the Reuters Corpus Volume I (RCV1) dataset. It is a multilabel dataset in which each data point can belong to multiple classes. The total number of classes is 103 . Each data point has a dimensionality of 47,236.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X, y = fetch_rcv1(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(d) Select 500 data points from RCV1 that belong to at least one of the first three classes. Use Achlioptas' method to project the selected data points to a lower-dimensional space. Then implement and visualize a matrix that stores the differences between distances of data points in the original space and in the lower-dimensional projection.\n",
        "\n",
        "Note that the type of `X` and `y` is not a `numpy` array, but a `scipy` sparse matrix. A sparse matrix is a matrix that contains mostly 0 and uses special algorithms to compute more efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first convert `y` to a `numpy` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = y.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we take first 500 samples from `X` that belong to the first three classes, then apply random projection. We can see that the Euclidean distance is preserved well: the original distances and their corresponding after-projection distances are mostly the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = X[(y[:, 0] == 1) | (y[:, 1] == 1) | (y[:, 2] == 1)][:500, :]\n",
        "R = generate_random_projection(X.shape[1], 5000)\n",
        "X_projected = X @ R\n",
        "\n",
        "dists = euclidean_distances(X, squared=True).ravel()\n",
        "nonzero = dists != 0\n",
        "dists = dists[nonzero]\n",
        "dists_rp = euclidean_distances(X_projected, squared=True).ravel()[nonzero]\n",
        "\n",
        "source = pd.DataFrame({\"orginal\": dists, \"projected\": dists_rp})\n",
        "alt.Chart(source.sample(5000)).mark_circle(opacity=0.5, size=1).encode(\n",
        "    x=\"orginal:Q\",\n",
        "    y=\"projected:Q\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}